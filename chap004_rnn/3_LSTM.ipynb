{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3b1b72-6068-423d-a080-65c5019a5426",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style=\"color:#153462; \n",
    "          font-weight: bold; \n",
    "          font-size: 30px; \n",
    "          font-family: Gill Sans, sans-serif; \n",
    "          text-align: center;\">\n",
    "          Definations Of Machine Learning</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18f19d-a574-444b-9323-5a17cb7bd0c4",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">Long Short Term Memory (LSTM)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9973e98b-7ed4-4ab0-92d8-6555b990e67f",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture that was specifically\n",
    "       designed to address the limitations of traditional RNNs in capturing long-range dependencies in sequential data.\n",
    "       The primary purpose of LSTM is to enable the effective modeling and prediction of sequences, making it\n",
    "       particularly useful in various tasks involving time series data, natural language processing, speech recognition,\n",
    "       and more.  Here are some key purposes and advantages of LSTM <br>  \n",
    "       <ol>\n",
    "        <li>\n",
    "            <strong>Handling Long-Term Dependencies:</strong> One of the main problems with vanilla RNNs is their\n",
    "            inability to effectively capture and learn long-range dependencies in sequences. LSTMs were developed\n",
    "            to address this issue by introducing a more sophisticated memory mechanism. They can remember information\n",
    "            over longer time intervals, making them suitable for tasks where context from distant past time steps is\n",
    "            crucial.\n",
    "        </li>\n",
    "        <li>\n",
    "            <strong>Learning and Forgetting:</strong> LSTMs have a mechanism to learn and forget information from\n",
    "            previous time steps. This is accomplished through gates (input, forget, and output gates) that control\n",
    "            the flow of information through the cell state, allowing the network to retain important information \n",
    "            while discarding irrelevant or outdated data.\n",
    "        </li>\n",
    "        <li>\n",
    "            <strong>Sequential Data Modeling:</strong> LSTMs excel at modeling and processing sequential data, such\n",
    "            as time series, natural language, and speech. They are widely used in applications like machine\n",
    "            translation, speech recognition, sentiment analysis, and more, where understanding the context and\n",
    "            dependencies between elements in the sequence is essential.\n",
    "        </li>\n",
    "        <li>\n",
    "            <strong>Reducing the Vanishing Gradient Problem:</strong> LSTMs mitigate the vanishing gradient problem\n",
    "            that affects traditional RNNs during training. This problem arises when gradients become very small as \n",
    "            they are backpropagated through many time steps. LSTMs use their gating mechanisms to control the flow \n",
    "            of gradients, making it easier to train deep networks on long sequences.\n",
    "        </li>\n",
    "        <li>\n",
    "            <strong>Robustness to Sequence Length:</strong> LSTMs can handle sequences of varying lengths, which is\n",
    "            valuable in real-world applications where input sequences can be of different lengths. This flexibility \n",
    "            is achieved through dynamic unrolling during training.\n",
    "        </li>\n",
    "        <li><strong>Versatility:</strong> LSTMs are adaptable and can be applied to various tasks, including\n",
    "            sequence prediction, text generation, sentiment analysis, anomaly detection, and more. Researchers\n",
    "            and practitioners continue to explore and develop new applications for LSTMs.\n",
    "        </li>\n",
    "       </ol>\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd06fc8-05f7-4a2a-90a6-a22cd4a0aaf9",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">Real World Example For RNN vs LSTM</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e58ee1a-e5d0-4842-9896-dbf8fb80bd21",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4>Memory Duration in Image Recognition</h4>\n",
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "    <font size=3>\n",
    "          Consider a scenario where you are shown an image, and after a short duration of 2 minutes, you are asked to recall details about\n",
    "          that image. In this situation, you would likely remember the image's content with relative ease. However, if the same question is\n",
    "          posed to you several days later, the information might have become vague or completely forgotten.\n",
    "    </font>\n",
    "</p>\n",
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "    <font size=3>\n",
    "          This distinction highlights the need for Recurrent Neural Networks (RNNs) when dealing with shorter-term memories and Long Short-Term\n",
    "          Memory networks (LSTMs) when a longer memory capacity is required. LSTMs excel at preserving information over extended periods, addressing\n",
    "          the challenge of retaining context and details over time.\n",
    "    </font>\n",
    "</p>\n",
    "<h4>Sequential Context in Movie Scene Recognition</h4>\n",
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "    <font size=3>\n",
    "          Imagine watching a movie without any prior knowledge of its title, such as \"Justice League.\" During one frame, you spot Ben Affleck,\n",
    "          leading you to speculate that it might be a Batman movie. In another frame, you see Gal Gadot, suggesting a connection to Wonder Woman.\n",
    "          However, as you continue to watch subsequent frames, your certainty grows that this is, indeed, \"Justice League.\"\n",
    "    </font>\n",
    "</p>\n",
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "    <font size=3>\n",
    "          This ability to gather context and make informed predictions based on information acquired from past frames characterizes the\n",
    "          functionality of LSTMs. LSTMs, like your cognitive process in this scenario, excel at retaining and utilizing sequential information\n",
    "          to make more accurate predictions or classifications over time.\n",
    "    </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0108b1-32c6-434c-99a6-f6250d968ec0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">Architecture of LSTM</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cdd4ba-7cbf-4184-8f1d-01b6de150b24",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       LSTMs deal with both Long Term Memory (LTM) and Short Term Memory (STM) and for making the calculations simple\n",
    "       and effective it uses the concept of gates.\n",
    "       <ul>\n",
    "           <li><b>Forget Gate</b>: LTM goes to forget gate and it forgets information that is not useful.</li>\n",
    "           <li>\n",
    "               <b>Learn Gate</b>: Event ( current input ) and STM are combined together so that necessary information\n",
    "               that we have recently learned from STM can be applied to the current input.\n",
    "           </li>\n",
    "           <li>\n",
    "               <b>Remember Gate</b>: LTM information that we havenâ€™t forget and STM and Event are combined together in\n",
    "               Remember gate which works as updated LTM.\n",
    "           </li>\n",
    "           <li><b>Use Gate</b>: This gate also uses LTM, STM, and Event to predict the output of the current event which works as an updated STM.</li>\n",
    "       </ul>\n",
    "   </font>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53c9ef8-2464-4e66-bb81-d2d05d93ae3c",
   "metadata": {},
   "source": [
    "<img src=\"images/LSTM/lstm_architecutre.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc3e157-ae42-49c9-a196-ba8eddf2fbfd",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       The above figure shows the simplified architecture of LSTMs. The actual mathematical architecture of LSTM\n",
    "       is represented using the following figure:\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef5d67-bd1e-4987-b625-f82e6fddff14",
   "metadata": {},
   "source": [
    "<img src=\"images/LSTM/mathematical_lstm.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6aec1c-8f6f-49c3-ae33-a953715216eb",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">Breaking Down the Architecture of LSTM</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2032f6c-4675-4ec1-b3b2-5fd9d093bbed",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The Learn Gate\n",
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "      Takes Event ( $E_{t}$ ) and Previous Short Term Memory ( $STM_{t-1}$ ) as input and keeps only relevant\n",
    "      information for prediction.\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e491a05-0b2e-48f5-978e-33855c398cbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"images/LSTM/learning_gate_lstm.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2649a32-7403-4fd9-835d-17b93120697e",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Mathematicall we can write like below:\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310dd361-0246-497f-90cf-2194f59ee46b",
   "metadata": {},
   "source": [
    "<img src=\"images/LSTM/mathematical_learning_lstm.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba445e-6ea0-49c3-b9cb-2586cdc4823a",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       <ul>\n",
    "           <li>\n",
    "               Previous Short Term Memory $STM_{t-1}$ and Current Event vector $E_{t}$ are joined together \n",
    "               $[STM_{t-1}, E_{t}]$ and multiplied with the weight matrix $W_{n}$ having some bias which is \n",
    "               then passed to $tanh$ ( hyperbolic Tangent ) function to introduce non-linearity to it,\n",
    "               and finally creates a matrix $N_{t}$.\n",
    "           </li>\n",
    "           <li>\n",
    "               For ignoring insignificant information we calculate one Ignore Factor $i_{t}$, for which we join Short\n",
    "               Term Memory $STM_{t-1}$ and Current Event vector $E_{t}$ and multiply with weight matrix $W_{i}$ and \n",
    "               passed through Sigmoid activation function with some bias.\n",
    "           </li>\n",
    "           <li>\n",
    "               Learn Matrix $N_{t}$ and Ignore Factor $i_{t}$ is multiplied together to produce learn gate result.\n",
    "           </li>\n",
    "       </ul>\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6b99e-264a-4100-9c5d-c8455dd03751",
   "metadata": {},
   "source": [
    "#### The Forget Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014cddb6-b080-4c55-8382-e3ffce599acd",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Takes Previous Long Term Memory ( LTMt-1 ) as input and decides on which information should be kept and\n",
    "       which to forget.\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a45e1-a988-4419-bd6b-a5999e42b34c",
   "metadata": {},
   "source": [
    "<img src=\"images/LSTM/forget_gate_lstm.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061441b0-4376-4f9c-b4f4-20e1380c8e4f",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Mathetical representation of the Forget Gate:\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7c454-ca26-4601-bba4-a0f336438fe5",
   "metadata": {},
   "source": [
    "<img src=\"images/LSTM/math-forget_gate.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdecc47-f661-419b-9caf-1843bb28307d",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       <ul>\n",
    "           <li>\n",
    "               Previous Short Term Memory $STM_{t-1}$ and Current Event vector $E_{t}$ are joined \n",
    "               together $[STM_{t-1}, E_{t}]$ and multiplied with the weight matrix $W_{f}$ and passed\n",
    "               through the Sigmoid activation function with some bias to form Forget Factor $f_{t}$.\n",
    "           </li>\n",
    "           <li>\n",
    "               Forget Factor $f_{t}$ is then multiplied with the Previous Long Term Memory $(LTM_{t-1})$\n",
    "               to produce forget gate output.\n",
    "           </li>\n",
    "       </ul>\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36630507-cefd-46b2-9c08-62c840d61c10",
   "metadata": {},
   "source": [
    "#### The Remember Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c615626-e20f-4c9d-a734-0da1437c33a2",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Combine Previous Short Term Memory $(STM_{t-1})$ and Current Event $(E_{t})$ to produce output.\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380652e-4965-4445-aa51-555e281a26e7",
   "metadata": {},
   "source": [
    "<img src=\"images/LSTM/remember_gate.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5017bd-b3d3-435b-9519-1490855918b7",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Mathematicall we can write like below:\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946c83be-bf6f-4db5-ac64-ba937f5f9f0c",
   "metadata": {},
   "source": [
    "<img src=\"images/LSTM/math_remember_gate.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3054d3b-5fe9-445f-92a9-4271fe46152c",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       The output of Forget Gate and Learn Gate are added together to produce an output of Remember Gate which\n",
    "       would be LTM for the next cell.\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdefc4f-30a8-4ecf-88bd-473790c08521",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">The Use Gate</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ad03a-6655-4aee-a61d-0676417acb2c",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Combine important information from Previous Long Term Memory and Previous Short Term Memory to create\n",
    "       STM for next and cell and produce output for the current event.\n",
    "   </font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228454b3-437c-4942-b03a-28faff331ff0",
   "metadata": {},
   "source": [
    "<img src=\"images/LSTM/use_gate.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7badae3c-d280-4f45-9ed9-a55bb2228ed1",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Mathematicall we can write like below:\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24e3fb-649f-48a8-a18d-c0a6cecfd010",
   "metadata": {},
   "source": [
    "<img src=\"images/LSTM/math_usage_gate.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120accfc-d523-45e1-8622-6557b75c1674",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       <ul>\n",
    "           <li>\n",
    "               Previous Long Term Memory $(LTM_{t-1})$ is passed through Tangent activation function with some\n",
    "               bias to produce $U_{t}$.\n",
    "           </li>\n",
    "           <li>\n",
    "               Previous Short Term Memory $(STM_{t-1})$ and Current Event $(E_{t})$ are joined together and passed\n",
    "               through Sigmoid activation function with some bias to produce $V_{t}$.\n",
    "           </li>\n",
    "           <li>\n",
    "               Output $U_{t}$ and $V_{t}$ are then multiplied together to produce the output of the use gate\n",
    "               which also works as STM for the next cell.\n",
    "           </li>\n",
    "       </ul>\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5ea343-aa58-415a-abe1-53462270ac53",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">Resources</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b824acc0-7856-4e26-b156-49e639b811d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "2. <a href=\"https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\"> Python implementation of LSTM</a>\n",
    "3. https://www.analyticsvidhya.com/blog/2021/01/understanding-architecture-of-lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e507656b-c529-4978-b461-32735cf7b183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
