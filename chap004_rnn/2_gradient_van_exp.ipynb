{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa6b2b6-4ab1-4943-827c-d2f8e73deb14",
   "metadata": {},
   "source": [
    "<p style=\"color:#153462; \n",
    "          font-weight: bold; \n",
    "          font-size: 30px; \n",
    "          font-family: Gill Sans, sans-serif; \n",
    "          text-align: center;\">\n",
    "          Gradient Vanish & Explode</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a7251e-b558-40b3-9395-7de61aeb907e",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Gradient vanishing and gradient explosion are two common issues that can occur during the training of deep neural networks(expecially in RNN), \n",
    "       particularly in the context of gradient-based optimization algorithms like stochastic gradient descent (SGD) or its variants. \n",
    "       These problems arise due to the nature of backpropagation and the chain rule in deep networks with many layers.<br>\n",
    "       Because of the multiplicative factor, $\\frac{\\partial h^\\left(t\\right)}{\\partial h^\\left(k\\right)}$, in computing the gradients of a loss function, the\n",
    "       so-called <i> Vanishing</i> and <i>Exploding</i> gradient problems arise.\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f02339-7b0b-4b10-8134-24d7ad9dec98",
   "metadata": {},
   "source": [
    "<img src=\"images/vani_explode.png\" alt=\"vanishing_explode\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca7470-1d29-46dc-9efc-d81094315a18",
   "metadata": {},
   "source": [
    "<span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">Gradient Vanishing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845756e2-2718-46ea-b0de-90df29c22547",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Gradient vanishing occurs when the gradients of the loss function with respect to the model's parameters become extremely small\n",
    "       as they are backpropagated through the network layers during the training process. As a result, the weights of the lower layers\n",
    "       (closer to the input) are not updated effectively, leading to slow or stalled learning. This is particularly problematic in deep\n",
    "       networks, where the gradients can diminish exponentially as they are propagated backward through the layers. In extreme cases,\n",
    "       the gradients can become so small that they practically have no impact on weight updates, effectively preventing the network \n",
    "       from learning complex patterns or representations.\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4cc063-4826-496d-8107-25d174c84ecd",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">Gradient Exploding</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6493e5ad-469a-45c0-b9a5-8c85bed50d22",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "        Gradient explosion, on the other hand, is the opposite problem. It occurs when the gradients of the loss function become extremely\n",
    "       large as they are propagated backward through the network layers. This can lead to weight updates that are so large that they cause\n",
    "       the model's parameters to diverge and result in numerical instability. This is especially prevalent in deep networks with large\n",
    "       weights, where gradients can grow exponentially as they are propagated backward.\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5de1cd-3d94-4577-8dd4-a3acd2252a87",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">Causes and Mitigation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df44f969-7e13-4a2e-9023-74f053297804",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "        Gradient vanishing often occurs in deep networks with sigmoid or hyperbolic tangent activation functions because these \n",
    "       functions can squash inputs into a limited range, leading to small gradients during backpropagation. To mitigate this issue,\n",
    "       alternative activation functions like ReLU (Rectified Linear Unit) or its variants are often used because they do not suffer\n",
    "       from the vanishing gradient problem to the same extent.<br><br>\n",
    "       Gradient explosion is typically addressed by using techniques like gradient clipping, which involves scaling the gradients if\n",
    "       they exceed a certain threshold during backpropagation. This prevents the gradients from becoming too large and causing \n",
    "       numerical instability. Additionally, careful weight initialization methods and normalization techniques like batch normalization\n",
    "       can help stabilize the training process and mitigate the risk of gradient explosion. \n",
    "   </font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d34e7-7619-4931-a745-c386d69bbe84",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">Resources</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a2c13-572f-4e3a-92c7-e8c1f08d2591",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li><a href=\"https://axon.cs.byu.edu/Dan/678/papers/Recurrent/Werbos.pdf\">Backpropagation through time: what it does and how to do it</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f222b20-7c77-4b64-abb0-a6f66c23e65b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
