{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65eae5c-329b-43a1-845e-c0e4ba060975",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style=\"color:#153462; \n",
    "          font-weight: bold; \n",
    "          font-size: 30px; \n",
    "          font-family: Gill Sans, sans-serif; \n",
    "          text-align: center;\">\n",
    "          Self Organizing Maps</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a0198b-9983-468f-8dd4-816f976bde39",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Self Organizing Map (SOM) also know as kohonen maps belongs to the unsupervized area of neural networks. SOM's are invented in 1980's by the professor Teuvo Kohonen. They are primarily used to produce a low-dimensional (typically one or two-dimensional) representation of a higher-dimensional data. SOM also represents clustering concept by grouping similar data together. Therefore it can be said that SOM <b>reduces data dimensions and displays similarities among data</b>.\n",
    "       <br><br>\n",
    "       One of the most interesting aspects of SOMs is that they learn to classify data without supervision. In supervised training techniques such as backpropagation where the training data consists of vector pairs (an input vector and a target vector). With this approach an input vector is presented to the network (typically a multilayer feedforward network) and the output is compared with the target vector. If they differ, the weights of the network are altered slightly to reduce the error in the output. This is repeated many times and with many sets of vector pairs until the network gives the desired output. Training a SOM however, requires no target vector. A SOM learns to classify the training data without any external supervision whatsoever.\n",
    "       <br><br>\n",
    "       The beauty of SOMs is that it preserves the topological relationship of the training dataset (or the input). Implying that the underlying properties of the input data are not affected even after the continuous change in the shape or the size of the figure. SOMs help to reveal correlations that are not easily identified.\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe0084-c43f-4176-bf35-7d59a1bafaa2",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">SOMs Architecture</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee736c2-7b49-49cf-9ceb-5b3b5511d631",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><img src=\"images\\2d-nodes-lattice.jpg\" alt=\"2d-nodes-lattice\" style=\"width: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148fbb57-644d-4b9e-85d6-da97b1a96747",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       The above image shows the 2-Dimentional SOM. The network is created from a 2D lattice of 'nodes'(showing in red), each of which is fully connected to the input layer. Here a very small Kohonen network of 4 X 4 nodes connected to the input layer (shown in green) representing a two dimensional vector (NOTE: Each column in the dataset represents a single dimension. Lets say if we have 5 rows then it would 5-dimentional).\n",
    "       <br><br>\n",
    "       Each node has a specific topological position (an x, y coordinate in the lattice) and contains a vector of weights of the same dimension as the input vectors. That is to say, if the training data consists of vectors, X,  of n dimensions $(X_1, X_2, X_3 ... X_n)$ then each node will contain a corresponding weight vector W, of n dimensions $(W_1, W_2, W_3 ... W_n)$\n",
    "       <br><br>\n",
    "       The lines connecting the nodes in above image are only there to represent adjacency and do not signify a connection as normally indicated when discussing a neural network. There are no lateral connections (let say a connection among nodes in the hidden layer) between nodes within the lattice.\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2b878-03db-4299-bb66-cb9d7e46665a",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">How SOM's works?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e332cc-f9ee-48a1-aa3e-0318c70f3163",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       A SOM does not need a target output to be specified unlike many other types of network. Instead, where the node weights match the input vector, that area of the lattice is selectively optimized to more closely resemble the data for the class the input vector is a member of. From an initial distribution of random weights, and over many iterations, the SOM eventually settles into a map of stable zones. Each zone is effectively a feature classifier, so you can think of the graphical output as a type of feature map of the input space. If you take another look at the trained network shown in  figure 1, the blocks of similar colour represent  the individual zones. Any new, previously unseen input vectors presented to the network will stimulate nodes in the zone with similar weight vectors. <br><br>\n",
    "Training occurs in several steps and over many iterations:\n",
    "<ul>\n",
    "    <li> Each node's weights are initialized.</li>\n",
    "    <li>A vector is chosen at random from the set of training data and presented to the lattice.</li>\n",
    "    <li>Every node is examined to calculate which one's weights are most like the input vector. <b>The winning node is commonly known as the Best Matching Unit (BMU).</b> </li>\n",
    "    <li>The radius of the neighbourhood of the BMU is now calculated. This is a value that starts large, typically set to the 'radius' of the lattice,  but diminishes each time-step. Any nodes found within this radius are deemed to be inside the BMU's neighbourhood.</li>\n",
    "    <li>Each neighbouring node's (the nodes found in step 4) weights are adjusted to make them more like the input vector. The closer a node is to the BMU, the more its weights get altered.</li>\n",
    "    <li>Repeat step 2 for N iterations.</li>\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f49a8-3361-44ef-83d5-5a0bddf97cf1",
   "metadata": {},
   "source": [
    " <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">Calculating the Best Matching Unit</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f407039d-a74b-48a7-aad6-13cdafccaf6c",
   "metadata": {},
   "source": [
    "<center><img src=\"images\\best-matching-unit.jpg\" alt=\"best-matching-unit\" style=\"width: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdf923d-e636-49d7-8c25-1e4e762a792e",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       To determine the best matching unit, one method is to iterate through all the nodes and calculate the Euclidean distance between each node's weight vector and the current input vector. The node with a weight vector closest to the input vector is tagged as the BMU. The Euclidean distance is given as: $Dist = \\sum_{i=0}^{i=n} (X_i - W_i)^2$ <br><br>\n",
    "       As an example, to calculate the distance between the vector for the colour red (1, 0, 0) with an arbitrary weight vector (0.1, 0.4, 0.5)<br>\n",
    "distance $= sqrt( (1 - 0.1)^2 + (0 - 0.4)^2+ (0 - 0.5)^2)$<br>\n",
    "         $= sqrt( (0.9)^2 + (-0.4)^2+ (-0.5)^2 )$<br>\n",
    "         $= sqrt( 0.81 + 0.16+ 0.25 )$<br>\n",
    "         $= sqrt(1.22)$<br>\n",
    "distance  = 1.106\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79af37c-3711-483f-852c-6c01e6196273",
   "metadata": {},
   "source": [
    "For remaining methematical calculation and explanation read:  http://www.ai-junkie.com/ann/som/som1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1555731b-24d5-4e39-bb49-ff5e907b2f50",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">Useful Resources</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40026db8-403e-46be-8982-b71d86b0e32a",
   "metadata": {
    "tags": []
   },
   "source": [
    "-> http://www.ai-junkie.com/ann/som/som1.html <br>\n",
    "-> https://www.superdatascience.com/blogs/self-organizing-maps-soms-how-do-self-organizing-maps-learn-part-1<br>\n",
    "-> https://www.analyticsvidhya.com/blog/2021/09/beginners-guide-to-anomaly-detection-using-self-organizing-maps/ <br>\n",
    "-> https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1990-Kohonen-PIEEE.pdf <br>\n",
    "-> https://sites.pitt.edu/~is2470pb/Spring05/FinalProjects/Group1a/tutorial/som.html<br>\n",
    "-> https://www.latentview.com/blog/self-organizing-maps/<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7bb014-8a6b-4ef5-883c-5df7de4610f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
