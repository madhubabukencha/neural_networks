{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65eae5c-329b-43a1-845e-c0e4ba060975",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style=\"color:#153462; \n",
    "          font-weight: bold; \n",
    "          font-size: 30px; \n",
    "          font-family: Gill Sans, sans-serif; \n",
    "          text-align: center;\">\n",
    "          Self Organizing Maps</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a0198b-9983-468f-8dd4-816f976bde39",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Self Organizing Map (SOM) also know as kohonen maps belongs to the unsupervized area of neural networks. SOM's are invented in 1980's by the professor Teuvo Kohonen. They are primarily used to produce a low-dimensional (typically one or two-dimensional) representation of a higher-dimensional data. SOM also represents clustering concept by grouping similar data together. Therefore it can be said that SOM <b>reduces data dimensions and displays similarities among data</b>.\n",
    "       <br><br>\n",
    "       One of the most interesting aspects of SOMs is that they learn to classify data without supervision. In supervised training techniques such as backpropagation where the training data consists of vector pairs (an input vector and a target vector). With this approach an input vector is presented to the network (typically a multilayer feedforward network) and the output is compared with the target vector. If they differ, the weights of the network are altered slightly to reduce the error in the output. This is repeated many times and with many sets of vector pairs until the network gives the desired output. Training a SOM however, requires no target vector. A SOM learns to classify the training data without any external supervision whatsoever.\n",
    "       <br><br>\n",
    "       The beauty of SOMs is that it preserves the topological relationship of the training dataset (or the input). Implying that the underlying properties of the input data are not affected even after the continuous change in the shape or the size of the figure. SOMs help to reveal correlations that are not easily identified.\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe0084-c43f-4176-bf35-7d59a1bafaa2",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">SOMs Architecture</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee736c2-7b49-49cf-9ceb-5b3b5511d631",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><img src=\"images\\2d-nodes-lattice.jpg\" alt=\"2d-nodes-lattice\" style=\"width: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148fbb57-644d-4b9e-85d6-da97b1a96747",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       The above image shows the 2-Dimentional SOM. The network is created from a 2D lattice of 'nodes'(showing in red), each of which is fully connected to the input layer. Here a very small Kohonen network of 4 X 4 nodes connected to the input layer (shown in green) representing a two dimensional vector (NOTE: Each column in the dataset represents a single dimension. Lets say if we have 5 rows then it would 5-dimentional).\n",
    "       <br><br>\n",
    "       Each node has a specific topological position (an x, y coordinate in the lattice) and contains a vector of weights of the same dimension as the input vectors. That is to say, if the training data consists of vectors, X,  of n dimensions $(X_1, X_2, X_3 ... X_n)$ then each node will contain a corresponding weight vector W, of n dimensions $(W_1, W_2, W_3 ... W_n)$\n",
    "       <br><br>\n",
    "       The lines connecting the nodes in above image are only there to represent adjacency and do not signify a connection as normally indicated when discussing a neural network. There are no lateral connections (let say a connection among nodes in the hidden layer) between nodes within the lattice.\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2b878-03db-4299-bb66-cb9d7e46665a",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">How SOM's works?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e332cc-f9ee-48a1-aa3e-0318c70f3163",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       A SOM does not need a target output to be specified unlike many other types of network. Instead, where the node weights match the input vector, that area of the lattice is selectively optimized to more closely resemble the data for the class the input vector is a member of. From an initial distribution of random weights, and over many iterations, the SOM eventually settles into a map of stable zones. Each zone is effectively a feature classifier, so you can think of the graphical output as a type of feature map of the input space. If you take another look at the trained network shown in  figure 1, the blocks of similar colour represent  the individual zones. Any new, previously unseen input vectors presented to the network will stimulate nodes in the zone with similar weight vectors. <br><br>\n",
    "Training occurs in several steps and over many iterations:\n",
    "<ul>\n",
    "    <li><b>Step-1</b>: We start with a dataset composed of n_feature independent variables</li>\n",
    "    <li>\n",
    "        <b>Step-2</b>: We create a grid composed of nodes. Each node having weight vector\n",
    "        of n_feature elements\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Step-3</b>: Randomly initialize the values of weight vectors to small number close\n",
    "        to zero (but not zero)\n",
    "    </li>\n",
    "    <li><b>Step-4</b>: Select one random observation point from the dataset</li>\n",
    "    <li>\n",
    "        <b>Step-5</b>: Compute the Euclidean distance from this point to the different\n",
    "          neurons in the network \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Step-6</b>: Select the neuron that has the minimum distance to the point. This neuron\n",
    "        is called winning Node also known as <b>Best Matching Unit (BMU).</b>\n",
    "    </li>\n",
    "    <li><b>Step-7</b>: Update the weights of the winning node to move it closer to the point</li>\n",
    "    <li>\n",
    "        <b>Step-8</b>: Using a Gaussian neighbourhood function of mean the winning node, also\n",
    "        update the weight of the winning node neighbours to move them closer to the point. <b>The\n",
    "        neighbourhood radius is the sigma in the Gaussian function</b>\n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Step-9</b>: Repeat Steps 1 to 5 and update weights after each observation or after a batch\n",
    "        of observations until the network converges to the point where neighbourhood stop decreasing.\n",
    "    </li>\n",
    "</ul>\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f49a8-3361-44ef-83d5-5a0bddf97cf1",
   "metadata": {},
   "source": [
    " <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">Calculating the Best Matching Unit</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f407039d-a74b-48a7-aad6-13cdafccaf6c",
   "metadata": {},
   "source": [
    "<center><img src=\"images\\best-matching-unit.jpg\" alt=\"best-matching-unit\" style=\"width: 300px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdf923d-e636-49d7-8c25-1e4e762a792e",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       To determine the best matching unit, one method is to iterate through all the nodes and\n",
    "       calculate the Euclidean distance between each node's weight vector and the current input\n",
    "       vector. The node with a weight vector closest to the input vector is tagged as the BMU.\n",
    "       <br><br>\n",
    "       The Euclidean distance is given as: $Dist = \\sqrt{\\sum_{i=0}^{i=n} (X_i - W_i)^2}$\n",
    "       <br><br>\n",
    "       <img src=\"images\\som-calculation.png\" alt=\"som-calculation\" style=\"width: 600px;\"/>\n",
    "       <br>\n",
    "       From the above image we can say that Node 2 is the Best Matching Unit(BMU). As an example,\n",
    "       to calculate the distance at Node 2, lets assume in our dataset first rows has data like (1, 0, 0)\n",
    "       and we assume (0.1, 0.4, 0.5) are arbitrary weights at Node 2:<br>\n",
    "       distance $= sqrt( (1 - 0.1)^2 + (0 - 0.4)^2+ (0 - 0.5)^2)$<br>\n",
    "         $= sqrt( (0.9)^2 + (-0.4)^2+ (-0.5)^2 )$<br>\n",
    "         $= sqrt( 0.81 + 0.16+ 0.25 )$<br>\n",
    "         $= sqrt(1.22)$<br>\n",
    "       distance  = 1.106\n",
    "   </font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79af37c-3711-483f-852c-6c01e6196273",
   "metadata": {},
   "source": [
    "For remaining methematical calculation and explanation read:  http://www.ai-junkie.com/ann/som/som1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1555731b-24d5-4e39-bb49-ff5e907b2f50",
   "metadata": {},
   "source": [
    "### <span style=\"color:#3C4048; font-weight: bold; font-size: 18px; font-family: Gill Sans, sans-serif;\">Useful Resources</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40026db8-403e-46be-8982-b71d86b0e32a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<strong> Videos </strong><br>\n",
    "https://www.udemy.com/course/deeplearning/learn/lecture/6905316#overview<br>\n",
    "https://www.udemy.com/course/deeplearning/learn/lecture/6905318#overview<br>\n",
    "\n",
    "<strong>Articals</strong><br>\n",
    "-> https://www.superdatascience.com/blogs/self-organizing-maps-soms-how-do-self-organizing-maps-learn-part-1<br>\n",
    "-> https://www.analyticsvidhya.com/blog/2021/09/beginners-guide-to-anomaly-detection-using-self-organizing-maps/ <br>\n",
    "-> https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1990-Kohonen-PIEEE.pdf <br>\n",
    "-> https://sites.pitt.edu/~is2470pb/Spring05/FinalProjects/Group1a/tutorial/som.html<br>\n",
    "-> https://www.latentview.com/blog/self-organizing-maps/<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c7bb014-8a6b-4ef5-883c-5df7de4610f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "\n",
    "# Generate some sample data (e.g., random points in 3D space)\n",
    "data = np.random.rand(100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38e9252-6fa5-4e72-be3b-90f110b145e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the SOM\n",
    "som = MiniSom(x=10, y=10, input_len=3, sigma=1.0, learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89897bbf-5315-49a8-9dbb-e6ed98898e36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minisom.MiniSom at 0x1921466aad0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "som"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fcbbdf9-8c80-444d-91f7-c55b06c38f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the weights\n",
    "som.random_weights_init(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e4116f-645e-4df7-8bf0-1edad05b2526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45eed253-5a59-4534-8ec7-b3c9e9266398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the SOM\n",
    "som.train_random(data, num_iteration=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9f2eff1-1717-4378-9f67-a091a62bb74b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the result\n",
    "plt.figure(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4597246-b576-4587-8e09-fbac84ab5f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = som.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8da6bb9a-ea67-4c3b-9cc1-db411f858c61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c1c4a-fa38-4709-9b13-3abf9a9ad712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
