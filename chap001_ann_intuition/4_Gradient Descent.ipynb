{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#153462; \n",
    "          font-weight: bold; \n",
    "          font-size: 30px; \n",
    "          font-family: Gill Sans, sans-serif; \n",
    "          text-align: center;\">\n",
    "          Gradient Descent</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word;\">\n",
    "   <font size=3>\n",
    "       Gradient descent is an optimization algorithm that is used to minimize the error or cost function of a \n",
    "       neural network during the training process. It works by iteratively adjusting the weights and biases of\n",
    "       the neural network in the direction of the steepest descent of the cost function.<br><br>\n",
    "       There are two main types of gradient descent: batch gradient descent and stochastic gradient descent. \n",
    "       <b><i>Batch Gradient Descent</i></b> computes the gradient for the entire training dataset in each iteration, while\n",
    "       <b><i>Stochastic Gradient Descent(SGD)</i></b> computes the gradient for a randomly selected sample from the dataset.\n",
    "       SGD does not require for the cost function to be convex and we update weight after every single row. SGD helps in\n",
    "       finding global minimum rather than finding local minimum.Stochastic gradient descent is typically faster and\n",
    "       more efficient than batch gradient descent, especially\n",
    "       for large datasets.<br><br>\n",
    "       Gradient descent is a crucial component of neural network training because it enables the network to\n",
    "       learn the optimal weights and biases for making accurate predictions. By minimizing the cost function,\n",
    "       the network can adjust its parameters to better fit the training data, and generalize well to new, \n",
    "       unseen data. Without gradient descent, training a neural network would be very difficult, if not impossible.       \n",
    "   </font>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>\n",
    "        <a href=\"https://iamtrask.github.io/2015/07/27/python-network-part2/\">\n",
    "            A Neural Network in 13 lines of Python (Part 2 - Gradient Descent)\n",
    "        </a>\n",
    "    </li>\n",
    "     <li>\n",
    "        <a href=\"https://static.latexstudio.net/article/2018/0912/neuralnetworksanddeeplearning.pdf\">\n",
    "            Neural Networks and Deep Learning by Michael Nielsen\n",
    "        </a>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
